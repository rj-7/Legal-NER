{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1f8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from utils import load_training_data, convert_biluo_scheme\n",
    "\n",
    "train_judgement_path = './NER_TRAIN/NER_TRAIN_JUDGEMENT.json'\n",
    "train_preamble_path = './NER_TRAIN/NER_TRAIN_PREAMBLE.json'\n",
    "\n",
    "dev_judgement_path = './NER_DEV/NER_DEV_JUDGEMENT.json'\n",
    "dev_preamble_path = './NER_DEV/NER_DEV_PREAMBLE.json'\n",
    "\n",
    "train_judgement_data = load_training_data(train_judgement_path, False)\n",
    "train_preamble_data = load_training_data(train_preamble_path, True)\n",
    "all_training_data = train_judgement_data + train_preamble_data\n",
    "shuffle(all_training_data)\n",
    "\n",
    "dev_judgement_data = load_training_data(dev_judgement_path, False)\n",
    "dev_preamble_data = load_training_data(dev_preamble_path, True)\n",
    "all_dev_data = dev_judgement_data + dev_preamble_data\n",
    "shuffle(all_dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c7685a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>PREAMBLE</th>\n",
       "      <th>ENTITIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This writ petition appears to have been drafte...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(51, 68, DATE), (150, 167, DATE), (202, 217, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Constitution Bench of the Apex Court in The St...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(26, 36, COURT), (44, 95, PRECEDENT)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He may choose to exercise, his. discretion to ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under these circumstances, the Tribunal was ju...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In this fog of authorities, however, a beacon ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(110, 182, PRECEDENT)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  PREAMBLE  \\\n",
       "0  This writ petition appears to have been drafte...     False   \n",
       "1  Constitution Bench of the Apex Court in The St...     False   \n",
       "2  He may choose to exercise, his. discretion to ...     False   \n",
       "3  Under these circumstances, the Tribunal was ju...     False   \n",
       "4  In this fog of authorities, however, a beacon ...     False   \n",
       "\n",
       "                                            ENTITIES  \n",
       "0  [(51, 68, DATE), (150, 167, DATE), (202, 217, ...  \n",
       "1             [(26, 36, COURT), (44, 95, PRECEDENT)]  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4                            [(110, 182, PRECEDENT)]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.DataFrame(all_training_data, columns = [\"TEXT\", \"PREAMBLE\", \"ENTITIES\"])\n",
    "training_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426f788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>PREAMBLE</th>\n",
       "      <th>ENTITIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At best, it can be said that because of the na...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(187, 198, PROVISION)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The observation in Md.Mohar Ali (supra) statin...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(19, 31, OTHER_PERSON)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W.P.No.9267 of 2017\\n\\n                       ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[(63, 97, COURT), (295, 306, JUDGE), (426, 444...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sri Sunil B.Ganu, learned counsel, would howev...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(4, 16, OTHER_PERSON), (68, 84, PROVISION)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suo Motu Writ Petition (C) No.1 of 2020, dated...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(0, 39, CASE_NUMBER), (47, 57, DATE), (58, 85...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  PREAMBLE  \\\n",
       "0  At best, it can be said that because of the na...     False   \n",
       "1  The observation in Md.Mohar Ali (supra) statin...     False   \n",
       "2  W.P.No.9267 of 2017\\n\\n                       ...      True   \n",
       "3  Sri Sunil B.Ganu, learned counsel, would howev...     False   \n",
       "4  Suo Motu Writ Petition (C) No.1 of 2020, dated...     False   \n",
       "\n",
       "                                            ENTITIES  \n",
       "0                            [(187, 198, PROVISION)]  \n",
       "1                           [(19, 31, OTHER_PERSON)]  \n",
       "2  [(63, 97, COURT), (295, 306, JUDGE), (426, 444...  \n",
       "3       [(4, 16, OTHER_PERSON), (68, 84, PROVISION)]  \n",
       "4  [(0, 39, CASE_NUMBER), (47, 57, DATE), (58, 85...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df = pd.DataFrame(all_dev_data, columns = [\"TEXT\", \"PREAMBLE\", \"ENTITIES\"])\n",
    "dev_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fde9d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>PREAMBLE</th>\n",
       "      <th>ENTITIES</th>\n",
       "      <th>BILUO_LABELS</th>\n",
       "      <th>BIO_LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This writ petition appears to have been drafte...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(51, 68, DATE), (150, 167, DATE), (202, 217, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-DATE, I-DATE, I-...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-DATE, I-DATE, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Constitution Bench of the Apex Court in The St...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(26, 36, COURT), (44, 95, PRECEDENT)]</td>\n",
       "      <td>[O, O, O, O, B-COURT, L-COURT, O, O, B-PRECEDE...</td>\n",
       "      <td>[O, O, O, O, B-COURT, I-COURT, O, O, B-PRECEDE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He may choose to exercise, his. discretion to ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under these circumstances, the Tribunal was ju...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In this fog of authorities, however, a beacon ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(110, 182, PRECEDENT)]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  PREAMBLE  \\\n",
       "0  This writ petition appears to have been drafte...     False   \n",
       "1  Constitution Bench of the Apex Court in The St...     False   \n",
       "2  He may choose to exercise, his. discretion to ...     False   \n",
       "3  Under these circumstances, the Tribunal was ju...     False   \n",
       "4  In this fog of authorities, however, a beacon ...     False   \n",
       "\n",
       "                                            ENTITIES  \\\n",
       "0  [(51, 68, DATE), (150, 167, DATE), (202, 217, ...   \n",
       "1             [(26, 36, COURT), (44, 95, PRECEDENT)]   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                            [(110, 182, PRECEDENT)]   \n",
       "\n",
       "                                        BILUO_LABELS  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, B-DATE, I-DATE, I-...   \n",
       "1  [O, O, O, O, B-COURT, L-COURT, O, O, B-PRECEDE...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                          BIO_LABELS  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-DATE, I-DATE, I-...  \n",
       "1  [O, O, O, O, B-COURT, I-COURT, O, O, B-PRECEDE...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = English()\n",
    "biluo_labels, bio_labels = convert_biluo_scheme(all_training_data, nlp)\n",
    "training_df[\"BILUO_LABELS\"] = biluo_labels\n",
    "training_df[\"BIO_LABELS\"] = bio_labels\n",
    "training_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9a9985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"W.P.No.9267 of 2017\n",
      "\n",
      "                             ...\" with entities \"[(63, 97, 'COURT'), (295, 306, 'JUDGE'), (426, 444...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Mr. Arun Bharadwaj, ld. CGSC, appearing for the Un...\" with entities \"[(4, 18, 'OTHER_PERSON'), (48, 62, 'ORG'), (111, 1...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "    Commercial Complex, Raj Bhavan Road, Hyderaba...\" with entities \"[(42, 51, 'GPE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"C/Lpa/1655/2019                             Judgme...\" with entities \"[(95, 129, 'COURT'), (466, 479, 'JUDGE'), (545, 56...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Honourable Dr. Justice B.Siva Sankara Rao         ...\" with entities \"[(23, 41, 'JUDGE'), (103, 125, 'PETITIONER'), (174...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"The said judgment was reported in M. Venkateswara ...\" with entities \"[(32, 96, 'PRECEDENT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"It is further urged that the aforesaid three decis...\" with entities \"[(113, 126, 'COURT'), (190, 209, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"In The High Court Of Judicature At Madras\n",
      "\n",
      "       ...\" with entities \"[(7, 41, 'COURT'), (232, 244, 'JUDGE'), (342, 432,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"In The High Court Of Karnataka At Bengaluru\n",
      "\n",
      "    D...\" with entities \"[(7, 43, 'COURT'), (138, 159, 'JUDGE'), (291, 307,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"It is in that context that the observations made b...\" with entities \"[(78, 85, 'OTHER_PERSON'), (205, 329, 'PRECEDENT')...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"This will be clarified in the instant case by comp...\" with entities \"[(60, 127, 'PROVISION')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"There is no mechanism for Patna High Court REQ. CA...\" with entities \"[(26, 42, 'COURT'), (48, 68, 'CASE_NUMBER'), (72, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"1\n",
      "\n",
      "\n",
      "                                              ...\" with entities \"[(120, 142, 'COURT'), (269, 283, 'PETITIONER'), (4...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"High Court Of Judicature At Allahabad\n",
      " \n",
      " \n",
      "\n",
      "Afr\n",
      " \n",
      "R...\" with entities \"[(0, 37, 'COURT'), (130, 168, 'PETITIONER'), (197,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"In The High Court For The State Of Telangana\n",
      "     ...\" with entities \"[(7, 45, 'COURT'), (190, 201, 'JUDGE'), (330, 348,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Non­Reportable\n",
      "                                 In...\" with entities \"[(55, 77, 'COURT'), (218, 236, 'PETITIONER'), (339...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"1\n",
      "\n",
      "                            Before The Madurai ...\" with entities \"[(42, 76, 'COURT'), (268, 280, 'JUDGE'), (543, 573...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"High Court Of Judicature At Allahabad, Lucknow Ben...\" with entities \"[(0, 52, 'COURT'), (175, 184, 'PETITIONER'), (213,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"High Court Of Judicature At Allahabad\n",
      " \n",
      " \n",
      "\n",
      "       ...\" with entities \"[(0, 37, 'COURT'), (315, 354, 'PETITIONER'), (371,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"(Manohar Lal Sharma14; Committee for Protection of...\" with entities \"[(1, 19, 'OTHER_PERSON'), (23, 68, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"The respondent M/s. Ultratech Cement Ltd. (hereina...\" with entities \"[(20, 41, 'RESPONDENT'), (179, 191, 'OTHER_PERSON'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"We may also point out that on an earlier occasion ...\" with entities \"[(92, 121, 'CASE_NUMBER'), (145, 173, 'PRECEDENT')...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"It is seen from the record that the arbitration pr...\" with entities \"[(88, 99, 'OTHER_PERSON'), (171, 180, 'GPE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      " Likewise, In the power of attorney (exhibit P/11...\" with entities \"[(70, 84, 'OTHER_PERSON'), (98, 109, 'OTHER_PERSON...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"68 Marata(J) final.doc available in the Sate of Ma...\" with entities \"[(48, 59, 'GPE'), (160, 170, 'COURT'), (193, 228, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      " (c) The aforesaid cheque was presented by the c...\" with entities \"[(93, 104, 'ORG'), (118, 127, 'GPE'), (226, 236, '...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "Digitally signed by:RAJENDER SINGH KARKI Signing...\" with entities \"[(22, 42, 'OTHER_PERSON'), (56, 66, 'DATE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>PREAMBLE</th>\n",
       "      <th>ENTITIES</th>\n",
       "      <th>BILUO_LABELS</th>\n",
       "      <th>BIO_LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At best, it can be said that because of the na...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(187, 198, PROVISION)]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The observation in Md.Mohar Ali (supra) statin...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(19, 31, OTHER_PERSON)]</td>\n",
       "      <td>[O, O, O, B-OTHER_PERSON, I-OTHER_PERSON, L-OT...</td>\n",
       "      <td>[O, O, O, B-OTHER_PERSON, I-OTHER_PERSON, I-OT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W.P.No.9267 of 2017\\n\\n                       ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[(63, 97, COURT), (295, 306, JUDGE), (426, 444...</td>\n",
       "      <td>[O, O, O, O, O, O, B-COURT, I-COURT, I-COURT, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-COURT, I-COURT, I-COURT, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sri Sunil B.Ganu, learned counsel, would howev...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(4, 16, OTHER_PERSON), (68, 84, PROVISION)]</td>\n",
       "      <td>[O, B-OTHER_PERSON, L-OTHER_PERSON, O, O, O, O...</td>\n",
       "      <td>[O, B-OTHER_PERSON, I-OTHER_PERSON, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suo Motu Writ Petition (C) No.1 of 2020, dated...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(0, 39, CASE_NUMBER), (47, 57, DATE), (58, 85...</td>\n",
       "      <td>[B-CASE_NUMBER, I-CASE_NUMBER, I-CASE_NUMBER, ...</td>\n",
       "      <td>[B-CASE_NUMBER, I-CASE_NUMBER, I-CASE_NUMBER, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  PREAMBLE  \\\n",
       "0  At best, it can be said that because of the na...     False   \n",
       "1  The observation in Md.Mohar Ali (supra) statin...     False   \n",
       "2  W.P.No.9267 of 2017\\n\\n                       ...      True   \n",
       "3  Sri Sunil B.Ganu, learned counsel, would howev...     False   \n",
       "4  Suo Motu Writ Petition (C) No.1 of 2020, dated...     False   \n",
       "\n",
       "                                            ENTITIES  \\\n",
       "0                            [(187, 198, PROVISION)]   \n",
       "1                           [(19, 31, OTHER_PERSON)]   \n",
       "2  [(63, 97, COURT), (295, 306, JUDGE), (426, 444...   \n",
       "3       [(4, 16, OTHER_PERSON), (68, 84, PROVISION)]   \n",
       "4  [(0, 39, CASE_NUMBER), (47, 57, DATE), (58, 85...   \n",
       "\n",
       "                                        BILUO_LABELS  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, B-OTHER_PERSON, I-OTHER_PERSON, L-OT...   \n",
       "2  [O, O, O, O, O, O, B-COURT, I-COURT, I-COURT, ...   \n",
       "3  [O, B-OTHER_PERSON, L-OTHER_PERSON, O, O, O, O...   \n",
       "4  [B-CASE_NUMBER, I-CASE_NUMBER, I-CASE_NUMBER, ...   \n",
       "\n",
       "                                          BIO_LABELS  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, B-OTHER_PERSON, I-OTHER_PERSON, I-OT...  \n",
       "2  [O, O, O, O, O, O, B-COURT, I-COURT, I-COURT, ...  \n",
       "3  [O, B-OTHER_PERSON, I-OTHER_PERSON, O, O, O, O...  \n",
       "4  [B-CASE_NUMBER, I-CASE_NUMBER, I-CASE_NUMBER, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biluo_labels, bio_labels = convert_biluo_scheme(all_dev_data, nlp)\n",
    "dev_df[\"BILUO_LABELS\"] = biluo_labels\n",
    "dev_df[\"BIO_LABELS\"] = bio_labels\n",
    "dev_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1488ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import encode_label_ids\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "train_biluo_labels = training_df[\"BILUO_LABELS\"].values.tolist()\n",
    "train_biluo_labels = list(np.concatenate(biluo_labels).flat)\n",
    "\n",
    "labels2i, ids_to_label = encode_label_ids(train_biluo_labels)\n",
    "\n",
    "training_sents = []\n",
    "training_pos = []\n",
    "for data in all_training_data:\n",
    "    tokens = nlp(data[0])\n",
    "    pos = [token.pos_ for token in tokens]\n",
    "    tokens = [str(token) for token in tokens]\n",
    "    training_sents.append(tokens)\n",
    "    training_pos.append(pos)\n",
    "\n",
    "dev_sents = []\n",
    "dev_pos = []\n",
    "for data in all_dev_data:\n",
    "    tokens = nlp(data[0])\n",
    "    pos = [token.pos_ for token in tokens]\n",
    "    tokens = [str(token) for token in tokens]\n",
    "    dev_sents.append(tokens)\n",
    "    dev_pos.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bae4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observation_dict():\n",
    "    tokensList = []\n",
    "    for data in all_training_data:\n",
    "        tokens = nlp(data[0])\n",
    "        tokens = [str(token) for token in tokens]\n",
    "        tokensList = tokensList + tokens\n",
    "    words = list(set(tokensList))\n",
    "    words.sort()\n",
    "    observation_dict = {word: i for i, word in enumerate(words)}\n",
    "    return observation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00df3845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id of the <unk> token: 45230\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "observation_dict = get_observation_dict()\n",
    "\n",
    "# we need to add the id for unknown word (<unk>) in our observations vocab\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "observation_dict[UNK_TOKEN] = len(observation_dict)\n",
    "print(\"id of the <unk> token:\", observation_dict[UNK_TOKEN])\n",
    "\n",
    "def encode(sentences: List[List[str]]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Using the observation_dict, convert the tokens to ids\n",
    "    unknown words take the id for UNK_TOKEN\n",
    "    \"\"\"\n",
    "    return [\n",
    "        [observation_dict[t] if t in observation_dict else observation_dict[UNK_TOKEN]\n",
    "            for t in sentence]\n",
    "        for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d071a47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anuj Bhavani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Anuj\n",
      "[nltk_data]     Bhavani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c66925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# TODO: Update this function to add more features\n",
    "#      You can check crf.py for how they are encoded, if interested.\n",
    "def make_features(text: List[str], tags: List[str]) -> List[List[int]]:\n",
    "    \"\"\"Turn a text into a feature vector.\n",
    "\n",
    "    Args:\n",
    "        text (List[str]): List of tokens.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: List of feature Lists.\n",
    "    \"\"\"\n",
    "    feature_lists = []\n",
    "    for i, token in enumerate(text):\n",
    "        feats = []\n",
    "        #tags = pos_tag(text)\n",
    "        # We add a feature for each unigram.\n",
    "        feats.append(f\"word={token}\")\n",
    "        # TODO: Add more features here\n",
    "        #feats.append(f\"pos={tags[i][1]}\")\n",
    "        feats.append(f\"pos={tags[i]}\")\n",
    "        if i == 0:\n",
    "            feats.append(f\"prev_word=<S>\");\n",
    "        else:\n",
    "            feats.append(f\"prev_word={text[i-1]}\")\n",
    "        if i == len(text) - 1:\n",
    "            feats.append(f\"next_word=<E>\")\n",
    "        else:\n",
    "            feats.append(f\"next_word={text[i+1]}\")\n",
    "        # We append each feature to a List for the token.\n",
    "        feature_lists.append(feats)\n",
    "    return feature_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d74a327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(sents: List[List[str]], tags: List[List[str]]) -> List[List[List[str]]]:\n",
    "    \"\"\"Turn the sentences into feature Lists.\n",
    "    \n",
    "    Eg.: For an input of 1 sentence:\n",
    "         [[['I','am','a','student','at','CU','Boulder']]]\n",
    "        Return list of features for every token for every sentence like:\n",
    "        [[\n",
    "         ['word=I',  'prev_word=<S>','pos=PRON',...],\n",
    "         ['word=an', 'prev_word=I'  , 'pos=VB' ,...],\n",
    "         [...]\n",
    "        ]]\n",
    "\n",
    "    Args:\n",
    "        sents (List[List[str]]): A List of sentences, which are Lists of tokens.\n",
    "\n",
    "    Returns:\n",
    "        List[List[List[str]]]: A List of sentences, which are Lists of feature Lists\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    for i in range(len(sents)):\n",
    "        feature = make_features(sents[i], tags[i])\n",
    "        feats.append(feature)\n",
    "\n",
    "#     for sent in sents:\n",
    "#         # Gets a List of Lists of feature strings\n",
    "#         feature = make_features(sent)\n",
    "#         print(feature)\n",
    "#         # TO DO: Get pos tags\n",
    "#         # sent_tags = get_pos(pos_tagger, [sent])[0]\n",
    "#         feats.append(feature)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca165b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crf import *\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# TODO: Implement the training loop\n",
    "# HINT: Build upon what we gave you for HW2.\n",
    "# See cell below for how we call this training loop.\n",
    "\n",
    "def training_loop(\n",
    "    num_epochs,\n",
    "    batch_size,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_features,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    labels2i,\n",
    "    pad_feature_idx\n",
    "):\n",
    "    # TODO: Zip the train features and labels\n",
    "    # TODO: Randomize them, while keeping them paired.\n",
    "    # TODO: Build batches\n",
    "    samples = list(zip(train_features, train_labels))\n",
    "    random.shuffle(samples)\n",
    "    batches = []\n",
    "    for i in range(0, len(samples), batch_size):\n",
    "        batches.append(samples[i:i+batch_size])\n",
    "    print(\"Training...\")\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch in tqdm(batches):\n",
    "            # Here we get the features and labels, pad them,\n",
    "            # and build a mask so that our model ignores PADs\n",
    "            # We have abstracted the padding from you for simplicity, \n",
    "            # but please reach out if you'd like learn more.\n",
    "            features, labels = zip(*batch)\n",
    "            features = pad_features(features, pad_feature_idx)\n",
    "            features = torch.stack(features)\n",
    "            # Pad the label sequences to all be the same size, so we\n",
    "            # can form a proper matrix.\n",
    "            labels = pad_labels(labels, labels2i[PAD_SYMBOL])\n",
    "            labels = torch.stack(labels)\n",
    "            mask = (labels != labels2i[PAD_SYMBOL])\n",
    "            # TODO: Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            # TODO: Run the model. Since we use the pytorch-crf model,\n",
    "            # our forward function returns the positive log-likelihood already.\n",
    "            # We want the negative log-likelihood. See crf.py forward method in NERTagger\n",
    "            loss = -model.forward(features, labels, mask)\n",
    "            # TODO: Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            # TODO: Update our coefficients in the direction of the gradient.\n",
    "            optimizer.step()\n",
    "            # TODO: Store the losses for logging\n",
    "            losses.append(loss.item())\n",
    "        # TODO: Log the average Loss for the epoch\n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # TODO: make dev predictions with the `predict()` function\n",
    "        dev_predictions = predict(model, dev_features)\n",
    "        # TODO: Compute F1 score on the dev set and log it.\n",
    "        dev_f1 = f1_score(dev_predictions, dev_labels, labels2i['O'])\n",
    "        print(f\"Dev F1 {dev_f1}\")\n",
    "        \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51c2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "Building features set!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10995/10995 [00:00<00:00, 38091.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Found 135234 features\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e62d1014c04b9ba1ea0a12f62d7b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 34.07039005188055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anuj Bhavani\\anaconda3\\lib\\site-packages\\torchcrf\\__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorCompare.cpp:402.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 tensor([0.1112])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62e55147b0a4bf18519dec3ab222b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 19.238854822031286\n",
      "Dev F1 tensor([0.1335])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a090de5d04344949c9c77f09137c4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss: 16.218920898298883\n",
      "Dev F1 tensor([0.1809])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd0a2f5d8e14eacb631dbbf55447016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss: 14.805929558221685\n",
      "Dev F1 tensor([0.2228])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81c1678f2174a6dba6a01b9d7aa7ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss: 13.69424912957258\n",
      "Dev F1 tensor([0.2539])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6c1d59ceba443f8882baffef09ca6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss: 13.251020854988763\n",
      "Dev F1 tensor([0.2994])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f22fc4e3044dc09b02c14943f4fb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss: 12.184567036323768\n",
      "Dev F1 tensor([0.3270])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7479cde01b4843d5b3652a7f022a3faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss: 11.829620380041211\n",
      "Dev F1 tensor([0.3884])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40014c197c724eed8a8b0195263e9a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss: 11.617172444975653\n",
      "Dev F1 tensor([0.4300])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbef261ab9524299badd73eac466b32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss: 11.06280397190604\n",
      "Dev F1 tensor([0.4445])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2109241760b742e6b683ef8fdf669184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss: 10.588711757299512\n",
      "Dev F1 tensor([0.4590])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3aea56e9db4ce889f393170c89817a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss: 10.65020798597225\n",
      "Dev F1 tensor([0.4770])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732eb89eac5e4659af61c6f7c6d8e57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss: 10.05143143271291\n",
      "Dev F1 tensor([0.4868])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ce4a2332504cbe9aa098bc525f5732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss: 10.046565933976062\n",
      "Dev F1 tensor([0.4929])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cafe621f3454eecb2c231e7942ff9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from crf import build_features_set\n",
    "from crf import make_features_dict\n",
    "from crf import encode_features, encode_labels\n",
    "from crf import NERTagger\n",
    "\n",
    "print(\"1\")\n",
    "train_features = featurize(training_sents, training_pos)\n",
    "print(\"2\")\n",
    "dev_features = featurize(dev_sents, dev_pos)\n",
    "\n",
    "# Get the full inventory of possible features\n",
    "print(\"3\")\n",
    "all_features = build_features_set(train_features)\n",
    "# Hash all features to a unique int.\n",
    "print(\"4\")\n",
    "features_dict = make_features_dict(all_features)\n",
    "# Initialize the model.\n",
    "print(\"5\")\n",
    "model = NERTagger(len(features_dict), len(labels2i))\n",
    "\n",
    "print(\"6\")\n",
    "encoded_train_features = encode_features(train_features, features_dict)\n",
    "print(\"7\")\n",
    "encoded_dev_features = encode_features(dev_features, features_dict)\n",
    "print(\"8\")\n",
    "train_tag_sents = training_df[\"BIO_LABELS\"].values.tolist()\n",
    "encoded_train_labels = encode_labels(train_tag_sents, labels2i)\n",
    "print(\"9\")\n",
    "dev_tag_sents = dev_df[\"BIO_LABELS\"].values.tolist()\n",
    "encoded_dev_labels = e ncode_labels(dev_tag_sents, labels2i)\n",
    "\n",
    "# TODO: Play with hyperparameters here.\n",
    "num_epochs = 45\n",
    "batch_size = 16\n",
    "LR=0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), LR)\n",
    "print(\"10\")\n",
    "model = training_loop(\n",
    "    num_epochs,\n",
    "    batch_size,\n",
    "    encoded_train_features,\n",
    "    encoded_train_labels,\n",
    "    encoded_dev_features,\n",
    "    encoded_dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    labels2i,\n",
    "    features_dict[PAD_SYMBOL]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d562fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tag_sents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
